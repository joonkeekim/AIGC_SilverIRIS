{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer, BertForNextSentencePrediction\n",
    "from transformers import ElectraTokenizerFast, ElectraModel, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained('/home/ubuntu/joonkee/pretraining/tokenizer_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N14']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.tokenize('N14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pretrain_data/transfered_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLanguageModelingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, sep_id: str='[SEP]', cls_id: str='[CLS]',\n",
    "                mask_id: str='[MASK]', pad_id: str=\"[PAD]\", seq_len: int=256, mask_frac: float=0.15, p: float=0.5):\n",
    "        \"\"\"Initiate language modeling dataset.\n",
    "        Arguments:\n",
    "            data (list): a tensor of tokens. tokens are ids after\n",
    "                numericalizing the string tokens.\n",
    "                torch.tensor([token_id_1, token_id_2, token_id_3, token_id1]).long()\n",
    "            vocab (sentencepiece.SentencePieceProcessor): Vocabulary object used for dataset.\n",
    "            p (float): probability for NSP. defaut 0.5\n",
    "        \"\"\"\n",
    "        super(BERTLanguageModelingDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.sep_id = tokenizer.sep_token_id\n",
    "        self.cls_id = tokenizer.cls_token_id\n",
    "        self.mask_id = tokenizer.mask_token_id\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "        self.p = p\n",
    "        self.mask_frac = mask_frac\n",
    "        self.mlm_probability = mask_frac\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        seq1 = self.data[i]['segmented_text']\n",
    "        seq2_idx = i\n",
    "        \n",
    "        # decide wheter use random next sentence or not for NSP task\n",
    "        if random.random() > self.p: # 틀린 데이터\n",
    "            is_next = torch.tensor(1)\n",
    "            if 'wrong' in self.data[i].keys():\n",
    "                rand_idx = random.randint(0,len(self.data[i]['wrong']))\n",
    "                if rand_idx:\n",
    "                    seq2 = self.data[seq2_idx]['wrong'][rand_idx-1]\n",
    "                else:\n",
    "                    while seq2_idx == i:\n",
    "                        seq2_idx = random.randint(0, len(data)-1)\n",
    "                    seq2 = self.data[seq2_idx]['equation']\n",
    "            else:\n",
    "                while seq2_idx == i:\n",
    "                    seq2_idx = random.randint(0, len(data)-1)\n",
    "                seq2 = self.data[seq2_idx]['equation']\n",
    "                # seq2 = self.data[seq2_idx]['equation']\n",
    "            # while seq2_idx == i:\n",
    "                # seq2_idx = random.randint(0, len(data))\n",
    "\n",
    "        else: # 맞는 데이터 \n",
    "            is_next = torch.tensor(0)\n",
    "            if 'right' in self.data[i].keys():\n",
    "                rand_idx = random.randint(0,len(self.data[i]['right']))\n",
    "                if rand_idx:\n",
    "                    seq2 = self.data[seq2_idx]['right'][rand_idx-1]\n",
    "                else:\n",
    "                    seq2 = self.data[seq2_idx]['equation']\n",
    "            else:\n",
    "                seq2 = self.data[seq2_idx]['equation']\n",
    "\n",
    "        # print(seq2)\n",
    "        encoded_dict = tok.encode_plus(\n",
    "                        [seq1, seq2],                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = self.seq_len,           # Pad & truncate all sentences.\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "        labels = encoded_dict['input_ids'].clone()\n",
    "        inputs = encoded_dict['input_ids'].clone()\n",
    "        # print(labels.shape)\n",
    "        # print(self.data[i]['id'], i)\n",
    "        special_tokens_mask = [tok.get_special_tokens_mask(val,already_has_special_tokens=True) for val in labels.tolist()]\n",
    "        special_tokens_mask = torch.tensor(special_tokens_mask,dtype=torch.bool)\n",
    "        # print(special_token_mask)\n",
    "        # print(labels)\n",
    "        # special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        mlm_train = inputs\n",
    "        mlm_target = labels\n",
    "        attn_masks = encoded_dict['attention_mask']\n",
    "        token_type_ids = encoded_dict['token_type_ids']\n",
    "        # print(mlm_train[:,:50], mlm_target[:,:50],sep='\\n')\n",
    "        # mlm_train, mlm_target, sentence embedding, NSP target\n",
    "        # print(mlm_train.shape, mlm_target.shape, attn_masks.shape, token_type_ids.shape, is_next.shape)\n",
    "        if mlm_train.shape[0] > 256:\n",
    "            print('?')\n",
    "        return mlm_train.squeeze(0), mlm_target.squeeze(0), attn_masks.squeeze(0), is_next\n",
    "        # return self.data[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.data:\n",
    "            yield x\n",
    "dataset = BERTLanguageModelingDataset(data=data,tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLM_NSP(nn.Module):\n",
    "    def __init__(self, voc_size:int=30000):\n",
    "        super(MLM_NSP, self).__init__()\n",
    "        d_model = 1024\n",
    "        # intermediate_hidden = 3072\n",
    "        self.linear_mlm1 = nn.Linear(d_model, d_model)\n",
    "        self.act = nn.GELU()\n",
    "        self.layer_norm = nn.LayerNorm(d_model,eps=1e-12)\n",
    "        self.linear_mlm2 = nn.Linear(d_model, voc_size)\n",
    "\n",
    "        self.linear_nsp1 = nn.Linear(d_model, d_model)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.linear_nsp2 = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        '''\n",
    "        param:\n",
    "            input: a batch of sequences of words\n",
    "            seg: Segmentation embedding for input tokens\n",
    "        dim:\n",
    "            input:\n",
    "                input: [B, S]\n",
    "                seg: [B, S]\n",
    "            output:\n",
    "                result: [B, S, V]\n",
    "        '''\n",
    "\n",
    "        output_mlm = self.linear_mlm1(input_seq) # [B, S, voc_size]\n",
    "        output_mlm = self.act(output_mlm) # [B, S, voc_size]\n",
    "        output_mlm = self.layer_norm(output_mlm) # [B, S, voc_size]\n",
    "        output_mlm = self.linear_mlm2(output_mlm) # [B, S, voc_size]\n",
    "\n",
    "        output_nsp = self.linear_nsp1(input_seq[:,0,:])\n",
    "        output_nsp = self.act2(output_nsp)\n",
    "        output_nsp = self.linear_nsp2(output_nsp)\n",
    "        # return output_nsp\n",
    "        return output_mlm, output_nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, voc_size, pretrained_path):\n",
    "        super(MyModel, self).__init__()\n",
    "        d_model = 1024\n",
    "        # intermediate_hidden = 3072\n",
    "        self.lm_model = RobertaModel.from_pretrained(pretrained_path)\n",
    "        self.mlm_nsp_model = MLM_NSP(voc_size)\n",
    "    def forward(self, mlm_train, attention_mask):\n",
    "        '''\n",
    "        param:\n",
    "            input: a batch of sequences of words\n",
    "            seg: Segmentation embedding for input tokens\n",
    "        dim:\n",
    "            input:\n",
    "                input: [B, S]\n",
    "                seg: [B, S]\n",
    "            output:\n",
    "                result: [B, S, V]\n",
    "        '''\n",
    "        output = self.lm_model(mlm_train, attention_mask=attention_mask).last_hidden_state\n",
    "        output = self.mlm_nsp_model(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def accuracy(log_pred, y_true):\n",
    "    y_pred = torch.argmax(log_pred, dim=1).to(y_true.device)\n",
    "    return (y_pred == y_true).to(torch.float)\n",
    "\n",
    "def train(model, dataloader, optimizer,valid_loader, total_leng, early_stop_cnt, scheduler, min_loss):\n",
    "    \n",
    "\n",
    "    mlm_epoch_loss = 0\n",
    "    nsp_epoch_loss = 0\n",
    "    # min_loss = 100\n",
    "    cnt = 0 # count length for avg loss\n",
    "    # early_stop_cnt = 0\n",
    "    stop = False\n",
    "    for batch, (mlm_train, mlm_target, attn_masks, is_next) in enumerate(tqdm(dataloader)):\n",
    "        # print(cnt)\n",
    "        # MLM task\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ###\n",
    "        # pdb.set_trace()\n",
    "        # elec_output = lm_model(mlm_train.to(DEVICE), attention_mask=attn_masks.to(DEVICE)).last_hidden_state\n",
    "        # output = mlm_nsp_model(elec_output)\n",
    "        # output_nsp = mlm_nsp_model(elec_output.to(DEVICE))\n",
    "        ###\n",
    "        output = model(mlm_train.to(DEVICE), attention_mask=attn_masks.to(DEVICE))\n",
    "        mlm_output = output[0].reshape(-1, output[0].shape[-1])\n",
    "        mlm_loss = criterion(mlm_output, mlm_target.to(DEVICE).reshape(-1)) # CE\n",
    "        nsp_loss = criterion(output[1], is_next.to(DEVICE)) # no need for reshape target\n",
    "        loss = mlm_loss+nsp_loss\n",
    "        # loss = nsp_loss\n",
    "        # torch.nn.utils.clip_grad_norm_(_loss.parameters(), 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # NSP tasks\n",
    "\n",
    "        mlm_epoch_loss += mlm_loss.item()\n",
    "        nsp_epoch_loss += nsp_loss.item()\n",
    "        # mlm_loss = 0\n",
    "        cnt += 1\n",
    "        if cnt % 20 == 0:\n",
    "            nsp_acc = accuracy(output[1], is_next).mean()\n",
    "            with open('log_mlm_nsp.txt', 'a') as f:\n",
    "                f.write(f'train : {cnt} step,  mlm : {mlm_loss.item():.2f}, nsp : {nsp_loss.item():.2f} nsp_acc : {nsp_acc:.2f}\\n')\n",
    "            print(f'train : {cnt} step,  mlm : {mlm_loss:.2f}, nsp : {nsp_loss.item():.2f} nsp_acc : {nsp_acc:.2f}\\n')\n",
    "        if cnt % 300 == 0:\n",
    "            mlm, nsp, acc = valid(model, valid_loader, total_leng)\n",
    "            # with open('log_rein.txt', 'a') as f:\n",
    "            #     f.write(f'validation : {cnt} step, early_stop_cnt{early_stop_cnt}, mlm : {mlm:.2f}, nsp : {nsp:.2f} nsp_acc : {acc:.2f}\\n')\n",
    "            # print(f'validation : {cnt} step, {early_stop_cnt}, mlm : {mlm:.2f}, nsp : {nsp:.2f} nsp_acc : {acc:.2f}\\n')\n",
    "\n",
    "            if mlm+nsp<min_loss:\n",
    "                early_stop_cnt = 0\n",
    "                min_loss = mlm+nsp\n",
    "                # print('min loss:',min_loss)\n",
    "                model.module.lm_model.save_pretrained('pretrained_mlm_nsp')\n",
    "            else:\n",
    "                early_stop_cnt += 1\n",
    "            with open('log_mlm_nsp.txt', 'a') as f:\n",
    "                f.write(f'validation : {cnt} step, early_stop_cnt{early_stop_cnt}, mlm : {mlm:.2f}, nsp : {nsp:.2f} nsp_acc : {acc:.2f} min_loss : {min_loss:.2f}\\n')\n",
    "            print(f'validation : {cnt} step, {early_stop_cnt}, mlm : {mlm:.2f}, nsp : {nsp:.2f} nsp_acc : {acc:.2f} min_loss : {min_loss:.2f}\\n')\n",
    "            if early_stop_cnt > 10:\n",
    "                stop = True\n",
    "                break\n",
    "        scheduler.step()\n",
    "    return mlm_epoch_loss / cnt, nsp_epoch_loss / cnt, stop, early_stop_cnt, min_loss\n",
    "\n",
    "def valid(model, dataloader, total_leng):\n",
    "    model.eval()\n",
    "    mlm_epoch_loss = 0\n",
    "    nsp_epoch_loss = 0\n",
    "    nsp_acc = 0\n",
    "    with torch.no_grad():\n",
    "        cnt = 0 # count length for avg loss\n",
    "        for batch, (mlm_train, mlm_target, attn_masks, is_next) in enumerate(tqdm(dataloader)):\n",
    "            # MLM task\n",
    "            ###\n",
    "            # elec_output = lm_model(mlm_train.to(DEVICE), attention_mask=attn_masks.to(DEVICE)).last_hidden_state\n",
    "            # output = mlm_nsp_model(elec_output)\n",
    "            # output_nsp = mlm_nsp_model(elec_output.to(DEVICE))\n",
    "            ###\n",
    "            # pdb.set_trace()\n",
    "            output = model(mlm_train.to(DEVICE), attention_mask=attn_masks.to(DEVICE))\n",
    "            mlm_output = output[0].reshape(-1, output[0].shape[-1])\n",
    "            mlm_loss = criterion(mlm_output, mlm_target.to(DEVICE).reshape(-1)) # CE\n",
    "            nsp_loss = criterion(output[1], is_next.to(DEVICE)) # no need for reshape target\n",
    "\n",
    "            # loss = mlm_loss+nsp_loss\n",
    "\n",
    "            mlm_epoch_loss += mlm_loss.item()\n",
    "            nsp_epoch_loss += nsp_loss.item()\n",
    "            cnt += 1\n",
    "            nsp_acc += accuracy(output[1], is_next).sum()\n",
    "                # step_nsp_acc = nsp_acc.mean()\n",
    "            #     with open('log_rein.txt', 'a') as f:\n",
    "            #         f.write(f'validation!! : {cnt} step,  mlm : {mlm_loss.item():.2f}, nsp : {nsp_loss.item():.2f} nsp_acc : {step_nsp_acc}\\n')\n",
    "            #     print(f'validation : {cnt} step,  mlm : {mlm_loss.item():.2f}, nsp : {nsp_loss.item():.2f} nsp_acc : {step_nsp_acc}\\n')\n",
    "\n",
    "    return mlm_epoch_loss / cnt, nsp_epoch_loss / cnt, nsp_acc/total_leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "model = MyModel(tok.vocab_size,'klue/roberta-large')\n",
    "# model.load_state_dict(torch.load('/home/ubuntu/joonkee/pretraining/pretrained_lm_mlm'))\n",
    "model.lm_model.from_pretrained('/home/ubuntu/joonkee/pretraining/pretrained_lm_mlm')\n",
    "model = nn.DataParallel(model)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "model.cuda()\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "train_dataset, val_dataset = random_split(dataset, [int(len(dataset)*0.95),len(dataset)- int(len(dataset)*0.95)])\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset,batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n",
    "optimizer_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1000, T_mult=1, eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad05347ae29a4b7c8ffb6b2883292f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.95 GiB (GPU 0; 31.72 GiB total capacity; 29.54 GiB already allocated; 918.88 MiB free; 29.65 GiB reserved in total by PyTorch)\nException raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7feb9fcac1e2 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1e64b (0x7febb126564b in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1f464 (0x7febb1266464 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x1faa1 (0x7febb1266aa1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7feaf594852e in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xf51329 (0x7feaf3d84329 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xf6b157 (0x7feaf3d9e157 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x10e9c7d (0x7feb2ab08c7d in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0x10e9f97 (0x7feb2ab08f97 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7feb2ac13a1a in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::native::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x49e (0x7feb2a891c3e in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x12880c1 (0x7feb2aca70c1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0x12c3863 (0x7feb2ace2863 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: at::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x101 (0x7feb2abf6b31 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: at::Tensor at::native::(anonymous namespace)::host_softmax_backward<at::native::(anonymous namespace)::LogSoftMaxBackwardEpilogue, true>(at::Tensor const&, at::Tensor const&, long, bool) + 0xa7 (0x7feaf51d41c7 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #15: at::native::log_softmax_backward_cuda(at::Tensor const&, at::Tensor const&, long, at::Tensor const&) + 0x65a (0x7feaf51be6da in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0xf3efa0 (0x7feaf3d71fa0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #17: <unknown function> + 0x11141d6 (0x7feb2ab331d6 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: at::_log_softmax_backward_data(at::Tensor const&, at::Tensor const&, long, at::Tensor const&) + 0x119 (0x7feb2abc1649 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: <unknown function> + 0x2ec639f (0x7feb2c8e539f in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: <unknown function> + 0x11141d6 (0x7feb2ab331d6 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: at::_log_softmax_backward_data(at::Tensor const&, at::Tensor const&, long, at::Tensor const&) + 0x119 (0x7feb2abc1649 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::generated::LogSoftmaxBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1d7 (0x7feb2c761057 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: <unknown function> + 0x3375bb7 (0x7feb2cd94bb7 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7feb2cd90400 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7feb2cd90fa1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7feb2cd89119 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7feba041134a in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #28: <unknown function> + 0xc819d (0x7febdd75d19d in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #29: <unknown function> + 0x76ba (0x7febe45c96ba in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #30: clone + 0x6d (0x7febe42ff41d in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6946bbdef027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmlm_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsp_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop_cnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-09005eb441e9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, valid_loader, total_leng, early_stop_cnt, scheduler, min_loss)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# loss = nsp_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# torch.nn.utils.clip_grad_norm_(_loss.parameters(), 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# NSP tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.95 GiB (GPU 0; 31.72 GiB total capacity; 29.54 GiB already allocated; 918.88 MiB free; 29.65 GiB reserved in total by PyTorch)\nException raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7feb9fcac1e2 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1e64b (0x7febb126564b in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1f464 (0x7febb1266464 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x1faa1 (0x7febb1266aa1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7feaf594852e in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xf51329 (0x7feaf3d84329 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xf6b157 (0x7feaf3d9e157 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x10e9c7d (0x7feb2ab08c7d in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0x10e9f97 (0x7feb2ab08f97 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7feb2ac13a1a in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::native::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x49e (0x7feb2a891c3e in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x12880c1 (0x7feb2aca70c1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0x12c3863 (0x7feb2ace2863 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: at::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x101 (0x7feb2abf6b31 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: at::Tensor at::native::(anonymous namespace)::host_softmax_backward<at::native::(anonymous namespace)::LogSoftMaxBackwardEpilogue, true>(at::Tensor const&, at::Tensor const&, long, bool) + 0xa7 (0x7feaf51d41c7 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #15: at::native::log_softmax_backward_cuda(at::Tensor const&, at::Tensor const&, long, at::Tensor const&) + 0x65a (0x7feaf51be6da in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0xf3efa0 (0x7feaf3d71fa0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #17: <unknown function> + 0x11141d6 (0x7feb2ab331d6 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: at::_log_softmax_backward_data(at::Tensor const&, at::Tensor const&, long, at::Tensor const&) + 0x119 (0x7feb2abc1649 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: <unknown function> + 0x2ec639f (0x7feb2c8e539f in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: <unknown function> + 0x11141d6 (0x7feb2ab331d6 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: at::_log_softmax_backward_data(at::Tensor const&, at::Tensor const&, long, at::Tensor const&) + 0x119 (0x7feb2abc1649 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::generated::LogSoftmaxBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1d7 (0x7feb2c761057 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: <unknown function> + 0x3375bb7 (0x7feb2cd94bb7 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7feb2cd90400 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7feb2cd90fa1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7feb2cd89119 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7feba041134a in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #28: <unknown function> + 0xc819d (0x7febdd75d19d in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #29: <unknown function> + 0x76ba (0x7febe45c96ba in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #30: clone + 0x6d (0x7febe42ff41d in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "early_stop_cnt = 0\n",
    "min_loss = 100\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    mlm_loss, nsp_loss, stop, early_stop_cnt, min_loss = train(model, dataloader, optimizer, valid_loader,len(dataset)- int(len(dataset)*0.95), early_stop_cnt,optimizer_scheduler, min_loss)\n",
    "    end_time = time.time()\n",
    "    if stop == True:\n",
    "        break\n",
    "    print('!!!!!',epoch, mlm_loss, nsp_loss, end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pretrained_lm4/tokenizer_config.json',\n",
       " 'pretrained_lm4/special_tokens_map.json',\n",
       " 'pretrained_lm4/vocab.txt',\n",
       " 'pretrained_lm4/added_tokens.json',\n",
       " 'pretrained_lm4/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.lm_model.save_pretrained('pretrained_lm5')\n",
    "# tok.save_pretrained('pretrained_lm4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lm_model.module.state_dict(), '/home/ubuntu/joonkee/pretraining/pretrained_lm4_torch/pretrained_lm4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pretrained_lm/tokenizer_config.json',\n",
       " 'pretrained_lm/special_tokens_map.json',\n",
       " 'pretrained_lm/vocab.txt',\n",
       " 'pretrained_lm/added_tokens.json',\n",
       " 'pretrained_lm/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.save_pretrained('pretrained_lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53a714998b4cda886d88c1f35ca09ebc6db63d3c7248d837ab3cd117369573cd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('pytorch_p36': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
