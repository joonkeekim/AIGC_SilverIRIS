{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer, BertForNextSentencePrediction\n",
    "from transformers import ElectraTokenizerFast, ElectraModel, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "tok = tok.save_pretrained('/home/ubuntu/joonkee/pretraining/tokenizer_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained('/home/ubuntu/joonkee/pretraining/tokenizer_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "elec = RobertaModel.from_pretrained('klue/roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def load_my_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # data = load_my_data('pretrain_data/Math23k_augmented.json')\n",
    "# # tmp_data = []\n",
    "# # for d in data:\n",
    "# #     if len(d['segmented_text']) + len(d['equation']) > 300:\n",
    "# #         continue\n",
    "# #     tmp_data.append(d)\n",
    "# f = open('pretrain_data/ape_math_augmented.json', encoding=\"utf-8\")\n",
    "# ch_data = json.load(f)\n",
    "# # f = open('pretrain_data/ape_math_augmented.json', encoding=\"utf-8\")\n",
    "# # ape_data = json.load(f)\n",
    "# data2 = load_my_data('pretrain_data/ape_math_pt.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data2\n",
    "# # len(ch_data),len(data)\n",
    "# data[0]\n",
    "# ch_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch_dic = {}\n",
    "# for i,d in enumerate(ch_data):\n",
    "#     ch_dic[d['id']] = d\n",
    "# dic ={}\n",
    "# for i,d in enumerate(data):\n",
    "#     if 'baidu_en' in d.keys():\n",
    "#         dic['ape'+d['id']] = d\n",
    "#     else:\n",
    "#         dic[d['id']] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 234012\n",
    "# dic[ch_data[idx]['id']], ch_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch_dic[ch_data[0]['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cnt = 0\n",
    "# # for d2 in data2:\n",
    "# #     # if d2['id'] == '1':\n",
    "# #         # print(d2)\n",
    "# #     if 'baidu_en' in d2.keys():\n",
    "# #         cnt+=1\n",
    "# # print(cnt)\n",
    "# # print(data[23161])\n",
    "# cnt = 0\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# for ch_d in tqdm(ch_data):\n",
    "#     # pdb.set_trace()\n",
    "#     # print(dic[ch_d['id']])\n",
    "#     if ch_d['id'] in dic.keys():\n",
    "#         if 'right' in ch_d.keys():\n",
    "#             dic[ch_d['id']]['right'] = (ch_d['right'])\n",
    "#         if 'wrong' in ch_d.keys():\n",
    "#             # print(dic[ch_d['id']])\n",
    "#             dic[ch_d['id']]['wrong'] = (ch_d['wrong'])\n",
    "#     else:\n",
    "#         # print(ch_d['id'])\n",
    "#         pass\n",
    "#     # for ch_d in ch_data:\n",
    "\n",
    "            \n",
    "#         # if d['id'] == ch_d['id'] and not 'baidu_en' in d2.keys():\n",
    "#             # d['segemented_text_kor'] = d2['segmented_text']\n",
    "#             # cnt += 1\n",
    "# # print(cnt)\n",
    "# #             # d['right'] = d2['right']\n",
    "# #             # d['wrong'] = d2['wrong']\n",
    "# # tmp_data = []\n",
    "# # for d in data:\n",
    "# #     if 'segemented_text_kor' not in d.keys():\n",
    "# #         continue\n",
    "# #     tmp_data.append(d)\n",
    "# # print(len(tmp_data))\n",
    "# # with open('pretrain_data/Math23k_ko_aug.json', 'w') as f:\n",
    "# #     json_string = json.dump(tmp_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # real_data = []\n",
    "# # # for k,v in dic.items():\n",
    "# # #     # pdb.set_trace()\n",
    "# # #     real_data.append(v)\n",
    "# # #     # print(v,k)\n",
    "# # with open('pretrain_data/ape_math_ko_aug.json', 'w') as f:\n",
    "# #     json_string = json.dump(real_data, f, indent=2, ensure_ascii=False)\n",
    "# a = [1,2,4, None]\n",
    "\n",
    "# None in a\n",
    "# len('((3003-3003)/(3003-(1/(3003/(3003/(3003/(3003/(3003/4))))))))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # f = open('pretrain_data/ape_math_ko_aug.json', encoding=\"utf-8\")\n",
    "# # data = json.load(f)\n",
    "# tmp_data = []\n",
    "# for d in data:\n",
    "#     if 'right' in d.keys():\n",
    "#         cnt = 0\n",
    "#         for i,r in enumerate(d['right']):\n",
    "#             if r == None:\n",
    "#                 # idx.append(i)\n",
    "#                 # pdb.set_trace()\n",
    "#                 print(d['right'])\n",
    "#                 cnt += 1\n",
    "#                 # d['right'].remove(None)\n",
    "#         for _ in range(cnt):\n",
    "#             d['right'].remove(None)\n",
    "#         if cnt:\n",
    "#             print('!!',d['right'])\n",
    "        \n",
    "#     if 'wrong' in d.keys():\n",
    "#         cnt = 0\n",
    "#         for i,r in enumerate(d['wrong']):\n",
    "#             if r == None:\n",
    "#                 print(d['wrong'])\n",
    "#                 # del d['wrong'][i]\n",
    "#                 # print(d['wrong'])\n",
    "#                 cnt += 1\n",
    "#         for _ in range(cnt):\n",
    "#             d['wrong'].remove(None)\n",
    "#         if cnt:\n",
    "#             print('!!',d['wrong'])\n",
    "#     tmp_data.append(d)\n",
    "# data = tmp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221488\n",
      "210291\n"
     ]
    }
   ],
   "source": [
    "f = open('pretrain_data/ape_math_ko_aug.json', encoding=\"utf-8\")\n",
    "data = json.load(f)\n",
    "tmp_data = []\n",
    "for d in data:\n",
    "    flag = False\n",
    "    if len(d['segmented_text'])>200 or len(d['equation']) > 30:\n",
    "        continue\n",
    "    if 'right' in d.keys():\n",
    "        if None in d['right']:\n",
    "            continue\n",
    "        for i,r in enumerate(d['right']):\n",
    "            if len(r) > 20:\n",
    "                flag = False\n",
    "                # del d['right'][i]\n",
    "    if 'wrong' in d.keys():\n",
    "        if None in d['wrong']:\n",
    "            continue\n",
    "        for i,r in enumerate(d['wrong']):\n",
    "            if len(r) > 20:\n",
    "                # del d['wrong'][i]\n",
    "                flag = False\n",
    "    if flag:\n",
    "        continue\n",
    "    tmp_data.append(d)\n",
    "print(len(data))\n",
    "print(len(tmp_data))\n",
    "data=tmp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    if d['equation'] is None:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLanguageModelingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, sep_id: str='[SEP]', cls_id: str='[CLS]',\n",
    "                mask_id: str='[MASK]', pad_id: str=\"[PAD]\", seq_len: int=256, mask_frac: float=0.15, p: float=0.5):\n",
    "        \"\"\"Initiate language modeling dataset.\n",
    "        Arguments:\n",
    "            data (list): a tensor of tokens. tokens are ids after\n",
    "                numericalizing the string tokens.\n",
    "                torch.tensor([token_id_1, token_id_2, token_id_3, token_id1]).long()\n",
    "            vocab (sentencepiece.SentencePieceProcessor): Vocabulary object used for dataset.\n",
    "            p (float): probability for NSP. defaut 0.5\n",
    "        \"\"\"\n",
    "        super(BERTLanguageModelingDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.sep_id = tokenizer.sep_token_id\n",
    "        self.cls_id = tokenizer.cls_token_id\n",
    "        self.mask_id = tokenizer.mask_token_id\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "        self.p = p\n",
    "        self.mask_frac = mask_frac\n",
    "        self.mlm_probability = mask_frac\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        seq1 = self.data[i]['segmented_text']\n",
    "        seq2_idx = i\n",
    "        # decide wheter use random next sentence or not for NSP task\n",
    "        if random.random() > self.p: # 틀린 데이터\n",
    "            is_next = torch.tensor(1)\n",
    "            if 'wrong' in self.data[i].keys():\n",
    "                rand_idx = random.randint(0,len(self.data[i]['wrong']))\n",
    "                if rand_idx:\n",
    "                    seq2 = self.data[seq2_idx]['wrong'][rand_idx-1]\n",
    "                else:\n",
    "                    while seq2_idx == i:\n",
    "                        seq2_idx = random.randint(0, len(data)-1)\n",
    "                    seq2 = self.data[seq2_idx]['equation']\n",
    "            else:\n",
    "                while seq2_idx == i:\n",
    "                    seq2_idx = random.randint(0, len(data)-1)\n",
    "                seq2 = self.data[seq2_idx]['equation']\n",
    "                # seq2 = self.data[seq2_idx]['equation']\n",
    "            # while seq2_idx == i:\n",
    "                # seq2_idx = random.randint(0, len(data))\n",
    "\n",
    "        else: # 맞는 데이터 \n",
    "            is_next = torch.tensor(0)\n",
    "            if 'right' in self.data[i].keys():\n",
    "                rand_idx = random.randint(0,len(self.data[i]['right']))\n",
    "                if rand_idx:\n",
    "                    seq2 = self.data[seq2_idx]['right'][rand_idx-1]\n",
    "                else:\n",
    "                    seq2 = self.data[seq2_idx]['equation']\n",
    "            else:\n",
    "                seq2 = self.data[seq2_idx]['equation']\n",
    "\n",
    "        # print(seq2)\n",
    "        encoded_dict = tok.encode_plus(\n",
    "                        [seq1, seq2],                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = self.seq_len,           # Pad & truncate all sentences.\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "        labels = encoded_dict['input_ids'].clone()\n",
    "        inputs = encoded_dict['input_ids'].clone()\n",
    "        # print(labels.shape)\n",
    "        # print(self.data[i]['id'], i)\n",
    "        special_tokens_mask = [tok.get_special_tokens_mask(val,already_has_special_tokens=True) for val in labels.tolist()]\n",
    "        special_tokens_mask = torch.tensor(special_tokens_mask,dtype=torch.bool)\n",
    "        # print(special_token_mask)\n",
    "        # print(labels)\n",
    "        # special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        '''\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "        '''\n",
    "        # def masking(data,mask_frac):\n",
    "        #     data = self.tokenizer.convert_tokens_to_ids(data)\n",
    "        #     data = torch.tensor(data).long().contiguous()\n",
    "        #     data_len = data.size(0)\n",
    "        #     ones_num = int(data_len * mask_frac)\n",
    "        #     zeros_num = data_len - ones_num\n",
    "        #     lm_mask = torch.cat([torch.zeros(zeros_num), torch.ones(ones_num)])\n",
    "        #     lm_mask = lm_mask[torch.randperm(data_len)]\n",
    "        #     data = data.masked_fill(lm_mask.bool(), self.mask_id)\n",
    "\n",
    "        #     return data\n",
    "        # print(self.tokenizer.tokenize(seq1))\n",
    "        # mlm_train = torch.cat([torch.tensor([self.cls_id]), masking(self.tokenizer.tokenize(seq1),self.mask_frac), torch.tensor([self.sep_id]), masking(self.tokenizer.tokenize(seq2),0), torch.tensor([self.sep_id])]).long().contiguous()\n",
    "        # mlm_train = torch.cat([mlm_train, torch.tensor([self.pad_id] * (self.seq_len - mlm_train.size(0)))]).long().contiguous()\n",
    "        mlm_train = inputs\n",
    "        mlm_target = labels\n",
    "        attn_masks = encoded_dict['attention_mask']\n",
    "        token_type_ids = encoded_dict['token_type_ids']\n",
    "        # print(mlm_train[:,:50], mlm_target[:,:50],sep='\\n')\n",
    "        # mlm_train, mlm_target, sentence embedding, NSP target\n",
    "        # print(mlm_train.shape, mlm_target.shape, attn_masks.shape, token_type_ids.shape, is_next.shape)\n",
    "        if mlm_train.shape[0] > 256:\n",
    "            print('?')\n",
    "        return mlm_train.squeeze(0), mlm_target.squeeze(0), attn_masks.squeeze(0), token_type_ids.squeeze(0), is_next\n",
    "        # return self.data[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.data:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0,   549,  2118, 27135,  5078,  2299,  2118,  5655,  2200,  3641,\n",
       "          2481,     4,  2038,     4,  2138,  4214,     4,    24,  8964,    16,\n",
       "             4,  2200,    22,  8964, 24094,  1077,  6460,     4, 18414,     4,\n",
       "          1513, 16809,     4,     2,    12,    12,  3879,    19,    22,    13,\n",
       "            14,    24,    13,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3879,\n",
       "         -100, 2037, -100, -100, 2460, -100, -100, -100, 4102, -100, -100, -100,\n",
       "         -100, -100, -100, 2138, -100, 1295, -100, -100,   18, -100, -100,   12,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = BERTLanguageModelingDataset(data=data,tokenizer=tok)\n",
    "dataset[2]\n",
    "# dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "# elec = elec.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for batch, (mlm_train, mlm_target, attn_masks, token_type_ids, is_next) in enumerate(dataloader):\n",
    "#     if batch % 100 == 0:\n",
    "#         print(batch)\n",
    "#     # pass\n",
    "#     # print(mlm_train.size())\n",
    "#     # print(mlm_train.size())\n",
    "#     # print(mlm_target.size())\n",
    "#     # print(attn_masks.size())\n",
    "#     # print(token_type_ids.size())\n",
    "#     # print(is_next.size())\n",
    "#     # print(elec(mlm_train.to(DEVICE), attention_mask=attn_masks.to(DEVICE), token_type_ids=token_type_ids.to(DEVICE)).last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLM_NSP(nn.Module):\n",
    "    def __init__(self, voc_size:int=30000):\n",
    "        super(MLM_NSP, self).__init__()\n",
    "        d_model = 1024\n",
    "        # intermediate_hidden = 3072\n",
    "        self.linear_mlm1 = nn.Linear(d_model, d_model)\n",
    "        self.act = nn.GELU()\n",
    "        self.layer_norm = nn.LayerNorm(d_model,eps=1e-12)\n",
    "        self.linear_mlm2 = nn.Linear(d_model, voc_size)\n",
    "\n",
    "        self.linear_nsp1 = nn.Linear(d_model, d_model)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.linear_nsp2 = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        '''\n",
    "        param:\n",
    "            input: a batch of sequences of words\n",
    "            seg: Segmentation embedding for input tokens\n",
    "        dim:\n",
    "            input:\n",
    "                input: [B, S]\n",
    "                seg: [B, S]\n",
    "            output:\n",
    "                result: [B, S, V]\n",
    "        '''\n",
    "\n",
    "        output_mlm = self.linear_mlm1(input_seq) # [B, S, voc_size]\n",
    "        output_mlm = self.act(output_mlm) # [B, S, voc_size]\n",
    "        output_mlm = self.layer_norm(output_mlm) # [B, S, voc_size]\n",
    "        output_mlm = self.linear_mlm2(output_mlm) # [B, S, voc_size]\n",
    "\n",
    "        output_nsp = self.linear_nsp1(input_seq[:,0,:])\n",
    "        output_nsp = self.act2(output_nsp)\n",
    "        output_nsp = self.linear_nsp2(output_nsp)\n",
    "        # return output_nsp\n",
    "        return output_mlm, output_nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def accuracy(log_pred, y_true):\n",
    "    y_pred = torch.argmax(log_pred, dim=1).to(y_true.device)\n",
    "    return (y_pred == y_true).to(torch.float)\n",
    "\n",
    "def train(lm_model, mlm_nsp_model, dataloader, lm_optimizer, head_optimizer, valid_loader, total_leng, early_stop_cnt):\n",
    "    \n",
    "\n",
    "    mlm_epoch_loss = 0\n",
    "    nsp_epoch_loss = 0\n",
    "    min_loss = 100\n",
    "    cnt = 0 # count length for avg loss\n",
    "    # early_stop_cnt = 0\n",
    "    stop = False\n",
    "    for batch, (mlm_train, mlm_target, attn_masks, token_type_ids, is_next) in enumerate(tqdm(dataloader)):\n",
    "        # print(cnt)\n",
    "        # MLM task\n",
    "        lm_model.train()\n",
    "        mlm_nsp_model.train()\n",
    "        lm_optimizer.zero_grad()\n",
    "        head_optimizer.zero_grad()\n",
    "        ###\n",
    "        # pdb.set_trace()\n",
    "        elec_output = lm_model(mlm_train.to(DEVICE), attention_mask=attn_masks.to(DEVICE)).last_hidden_state\n",
    "        output = mlm_nsp_model(elec_output)\n",
    "        # output_nsp = mlm_nsp_model(elec_output.to(DEVICE))\n",
    "        ###\n",
    "        mlm_output = output[0].reshape(-1, output[0].shape[-1])\n",
    "        mlm_loss = criterion(mlm_output, mlm_target.to(DEVICE).reshape(-1)) # CE\n",
    "        nsp_loss = criterion(output[1], is_next.to(DEVICE)) # no need for reshape target\n",
    "\n",
    "        # mlm_loss = criterion(output[0], mlm_target.to(DEVICE)) # CE\n",
    "        # nsp_loss = criterion(output[1], is_next.to(DEVICE)) \n",
    "\n",
    "        loss = mlm_loss+nsp_loss\n",
    "        # loss = nsp_loss\n",
    "        # torch.nn.utils.clip_grad_norm_(_loss.parameters(), 1)\n",
    "        loss.backward()\n",
    "        lm_optimizer.step()\n",
    "        head_optimizer.step()\n",
    "        # NSP tasks\n",
    "\n",
    "        mlm_epoch_loss += mlm_loss.item()\n",
    "        # mlm_loss = 0\n",
    "        nsp_epoch_loss += nsp_loss.item()\n",
    "        cnt += 1\n",
    "        if cnt % 20 == 0:\n",
    "            nsp_acc = accuracy(output[1], is_next).mean()\n",
    "            with open('log_rein.txt', 'a') as f:\n",
    "                f.write(f'train : {cnt} step,  mlm : {mlm_loss.item():.2f}, nsp : {nsp_loss.item():.2f} nsp_acc : {nsp_acc:.2f}\\n')\n",
    "            print(f'train : {cnt} step,  mlm : {mlm_loss:.2f}, nsp : {nsp_loss.item():.2f} nsp_acc : {nsp_acc:.2f}\\n')\n",
    "        if cnt % 100 == 0:\n",
    "            mlm, nsp, acc = valid(lm_model, mlm_nsp_head, valid_loader, total_leng)\n",
    "            with open('log_rein.txt', 'a') as f:\n",
    "                f.write(f'validation : {cnt} step, early_stop_cnt{early_stop_cnt}, mlm : {mlm:.2f}, nsp : {nsp:.2f} nsp_acc : {acc:.2f}\\n')\n",
    "            print(f'validation : {cnt} step, {early_stop_cnt}, mlm : {mlm:.2f}, nsp : {nsp:.2f} nsp_acc : {acc:.2f}\\n')\n",
    "\n",
    "            if mlm+nsp<min_loss:\n",
    "                early_stop_cnt = 0\n",
    "                min_loss = mlm+nsp\n",
    "                print('min loss:',min_loss)\n",
    "                lm_model.module.save_pretrained('pretrained_lm3')\n",
    "            else:\n",
    "                early_stop_cnt += 1\n",
    "            if early_stop_cnt > 10:\n",
    "                stop = True\n",
    "                break\n",
    "    return mlm_epoch_loss / cnt, nsp_epoch_loss / cnt, stop, early_stop_cnt\n",
    "\n",
    "def valid(lm_model, mlm_nsp_model, dataloader, total_leng):\n",
    "    lm_model.eval()\n",
    "    mlm_nsp_model.eval()\n",
    "\n",
    "    mlm_epoch_loss = 0\n",
    "    nsp_epoch_loss = 0\n",
    "    nsp_acc = 0\n",
    "    with torch.no_grad():\n",
    "        cnt = 0 # count length for avg loss\n",
    "        for batch, (mlm_train, mlm_target, attn_masks, token_type_ids, is_next) in enumerate(tqdm(dataloader)):\n",
    "            # MLM task\n",
    "            lm_optimizer.zero_grad()\n",
    "            head_optimizer.zero_grad()\n",
    "            ###\n",
    "            elec_output = lm_model(mlm_train.to(DEVICE), attention_mask=attn_masks.to(DEVICE)).last_hidden_state\n",
    "            output = mlm_nsp_model(elec_output)\n",
    "            # output_nsp = mlm_nsp_model(elec_output.to(DEVICE))\n",
    "            ###\n",
    "            # pdb.set_trace()\n",
    "            mlm_output = output[0].reshape(-1, output[0].shape[-1])\n",
    "            mlm_loss = criterion(mlm_output, mlm_target.to(DEVICE).reshape(-1)) # CE\n",
    "            nsp_loss = criterion(output[1], is_next.to(DEVICE)) # no need for reshape target\n",
    "\n",
    "            loss = mlm_loss+nsp_loss\n",
    "\n",
    "            mlm_epoch_loss += mlm_loss.item()\n",
    "            nsp_epoch_loss += nsp_loss.item()\n",
    "            cnt += 1\n",
    "            nsp_acc += accuracy(output[1], is_next).sum()\n",
    "                # step_nsp_acc = nsp_acc.mean()\n",
    "            #     with open('log_rein.txt', 'a') as f:\n",
    "            #         f.write(f'validation!! : {cnt} step,  mlm : {mlm_loss.item():.2f}, nsp : {nsp_loss.item():.2f} nsp_acc : {step_nsp_acc}\\n')\n",
    "            #     print(f'validation : {cnt} step,  mlm : {mlm_loss.item():.2f}, nsp : {nsp_loss.item():.2f} nsp_acc : {step_nsp_acc}\\n')\n",
    "\n",
    "    return mlm_epoch_loss / cnt, nsp_epoch_loss / cnt, nsp_acc/total_leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MLM_NSP(\n",
       "    (linear_mlm1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (act): GELU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (linear_mlm2): Linear(in_features=1024, out_features=32000, bias=True)\n",
       "    (linear_nsp1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (act2): Tanh()\n",
       "    (linear_nsp2): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lm_model = elec.to(DEVICE)\n",
    "# mlm_nsp_head = MLM_NSP(tok.vocab_size).to(DEVICE)\n",
    "# lm_model = elec\n",
    "# mlm_nsp_head = MLM_NSP(tok.vocab_size)\n",
    "# from parallel import DataParallelModel, DataParallelCriterion\n",
    "lm_model = nn.DataParallel(elec)\n",
    "mlm_nsp_head = nn.DataParallel(MLM_NSP(tok.vocab_size))\n",
    "# lm_model.cuda()\n",
    "# mlm_nsp_head.cuda()\n",
    "# lm_model = DataParallelModel(elec)\n",
    "# mlm_nsp_head = DataParallelModel(MLM_NSP(tok.vocab_size))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = DataParallelCriterion(criterion)\n",
    "lm_model.cuda()\n",
    "mlm_nsp_head.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict = tok.encode_plus(\n",
    "                        [data[16750]['segmented_text'], data[16750]['equation']],                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "type((elec(encoded_dict['input_ids'].to(DEVICE)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "train_dataset, val_dataset = random_split(dataset, [int(len(dataset)*0.95),len(dataset)- int(len(dataset)*0.95)])\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset,batch_size=32, shuffle=False)\n",
    "lm_optimizer = optim.Adam(lm_model.parameters(), lr=3e-5)\n",
    "head_optimizer = optim.Adam(mlm_nsp_head.parameters(), lr=1e-4)\n",
    "# you can also optimize the parameters like below:\n",
    "# optim.Adam(list(mlm_head.parameters()) + list(nsp_head.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "N_EPOCHS = 10\n",
    "\n",
    "\n",
    "early_stop_cnt = 0\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    mlm_loss, nsp_loss, stop, early_stop_cnt= train(lm_model, mlm_nsp_head, dataloader, lm_optimizer, head_optimizer,valid_loader,len(dataset)- int(len(dataset)*0.95), early_stop_cnt)\n",
    "    end_time = time.time()\n",
    "    if stop == True:\n",
    "        break\n",
    "    print('!!!!!',epoch, mlm_loss, nsp_loss, end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pretrained_lm4/tokenizer_config.json',\n",
       " 'pretrained_lm4/special_tokens_map.json',\n",
       " 'pretrained_lm4/vocab.txt',\n",
       " 'pretrained_lm4/added_tokens.json',\n",
       " 'pretrained_lm4/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_model.module.save_pretrained('pretrained_lm4')\n",
    "tok.save_pretrained('pretrained_lm4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lm_model.module.state_dict(), '/home/ubuntu/joonkee/pretraining/pretrained_lm4_torch/pretrained_lm4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pretrained_lm/tokenizer_config.json',\n",
       " 'pretrained_lm/special_tokens_map.json',\n",
       " 'pretrained_lm/vocab.txt',\n",
       " 'pretrained_lm/added_tokens.json',\n",
       " 'pretrained_lm/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.save_pretrained('pretrained_lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53a714998b4cda886d88c1f35ca09ebc6db63d3c7248d837ab3cd117369573cd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('pytorch_p36': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
